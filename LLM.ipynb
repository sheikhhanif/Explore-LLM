{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbQp3BHLRbisPW6auFC3FH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheikhhanif/Explore-LLM/blob/main/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM\n",
        "An LLM or large language model is a type of artificial intelligence (AI) that can process and understand human language. It is trained on a massive amount of text data, which allows it to learn the patterns and rules of language. This enables LLMs to perform a variety of tasks, such as generating text, translating languages, writing different kinds of creative content, and answering your questions in an informative way.\n",
        "\n",
        "Papers:\n",
        "- Attention Is All You Need: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "\n",
        "- BERT: https://arxiv.org/abs/1810.04805\n",
        "- GPT-1: https://www.mikecaptain.com/resources/pdf/GPT-1.pdf\n",
        "- LLaMA: https://arxiv.org/abs/2302.13971"
      ],
      "metadata": {
        "id": "MgVzyRrVTSGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are parameters\n",
        "The total number of parameters is the sum of all the weights and biases on the neural network. When calculating manually, different types of layers have different methods. The parameters on the Dense, Conv2d, or maybe LSTM layers are slightly different. The principle is the same, we only need to calculate the unit weight and bias."
      ],
      "metadata": {
        "id": "QT-yyLEfQX20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are Tokens\n",
        "\n",
        "In natural language processing (NLP), a token is a unit of text that has been extracted from a larger body of text. This extraction process is called tokenization. The larger body of text, often a sentence or a document, is broken down into smaller units, which can be words, subwords, or characters, depending on the level of granularity required for analysis. Usually, a token consists of 4 characters.\n"
      ],
      "metadata": {
        "id": "Lxj9KwbJXPsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is context window?\n",
        "The context window is the portion of the input sequence that the model takes into account to make predictions for the current word. The size of the context window determines how much historical information the model considers when predicting the next word. A larger context window allows the model to capture more long-range dependencies in the text but may also increase computational complexity.\n",
        "\n",
        "For example, if you have the sentence: \"The cat sat on the mat,\" and you're predicting the next word after \"sat,\" a context window of size 2 might include the words \"The cat,\" a context window of size 3 might include \"The cat sat,\" and so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rn1LvfPlYNvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applied LLM"
      ],
      "metadata": {
        "id": "eLnT92ilkeRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package installation"
      ],
      "metadata": {
        "id": "oGw3-Tq4kjew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o9cVPZ0SK4aD"
      },
      "outputs": [],
      "source": [
        "!pip install -q ctransformers langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading methods"
      ],
      "metadata": {
        "id": "ZQfrgQotTRM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import CTransformers\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "RSto0EkMMYUx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About LLaMA2\n",
        "Llama-2 is a 70B parameter large language model created by Meta AI. It is a factual language model, trained on a massive dataset of text and code. Llama-2 can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc. It will try its best to follow your instructions and complete your requests thoughtfully.\n",
        "\n",
        "5-bit quantized Llama-2 models are compressed versions of the original Llama-2 model that use less memory and are faster to run. This is done by reducing the precision of the model's parameters from 16 bits to 5 bits. This can lead to a small loss in accuracy, but it is usually not noticeable.\n",
        "\n",
        "There are several benefits to using 5-bit quantized Llama-2 models. First, they are much smaller than the original Llama-2 model. For example, the 70B parameter 5-bit quantized Llama-2 model is only 4.63 GB in size, compared to 13.5 GB for the original model. This makes it possible to run Llama-2 on devices with less memory, such as laptops and smartphones.\n",
        "\n",
        "Second, 5-bit quantized Llama-2 models are faster to run than the original model. This is because they require less computation to perform the same task. For example, the 70B parameter 5-bit quantized Llama-2 model can generate text twice as fast as the original model on a GPU.\n",
        "\n",
        "Third, 5-bit quantized Llama-2 models are more energy-efficient than the original model. This is because they require less power to run. This makes them ideal for use on mobile devices and other battery-powered devices.\n",
        "\n",
        "Overall, 5-bit quantized Llama-2 models are a good choice for users who need a smaller, faster, and more energy-efficient version of the original Llama-2 model. They are still very accurate and can be used for a variety of tasks, such as text generation, translation, and summarization."
      ],
      "metadata": {
        "id": "zYwcZUDAlQuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "XT8__xc6mSv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get started"
      ],
      "metadata": {
        "id": "DuykbSWcpTx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config={\"max_new_tokens\":512,\"context_length\":2048}"
      ],
      "metadata": {
        "id": "SXMqzrylMetj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm2 = CTransformers(model=\"TheBloke/Llama-2-7B-Chat-GGML\",model_file=\"llama-2-7b-chat.ggmlv3.q5_K_M.bin\",callbacks=[StreamingStdOutCallbackHandler()],config=config)"
      ],
      "metadata": {
        "id": "XJ0fBaq8Mkp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "[INST] <<sys>>\n",
        "Be precise with your answer.\n",
        "<</sys>>\n",
        "{text}[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template= template,input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "5yNOSi1DNAdU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llmchain2 = LLMChain(prompt=prompt,llm=llm2)"
      ],
      "metadata": {
        "id": "hI9v-ghVNU-P"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"\"\"\n",
        "Tell me about Retrieval augmented generation. only 2 sentences.\n",
        "\"\"\"\n",
        "\n",
        "response = llmchain2.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNVKFOInNbpg",
        "outputId": "7909a433-199c-458f-bac8-19a79d35bb14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval augmented generation (RAG) is a technique used in natural language processing (NLP) that combines the strengths of both retrieval-based and generative models to generate high-quality text. By incorporating a retrieval model into the generation process, RAG can better capture the nuances of language and produce more coherent and fluent text."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze a single long document: LangChain\n",
        "The AnalyzeDocumentChain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain.\n",
        "\n"
      ],
      "metadata": {
        "id": "R5F_eIkJpjQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import AnalyzeDocumentChain\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "YbWqMTUJrmKP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/arab-summit-gaza.txt\") as f:\n",
        "    arab_summit_gaza = f.read()"
      ],
      "metadata": {
        "id": "w-kpKlGqpqx5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = load_qa_chain(llm2, chain_type=\"map_reduce\")\n",
        "qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n",
        "qa_document_chain.run(\n",
        "    input_document=arab_summit_gaza,\n",
        "    question=\"what did the final statement call for?\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syH8nLjGrvke",
        "outputId": "ce300bb2-f745-4aff-da77-cdcc1c143403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "The Final Statement calls on the UN Security Council to take a decisive and binding decision that imposes a halt to the aggression and curbs the colonial occupation authority in its violation of international laws and legitimate resolutions. The Statement also calls on all countries to stop exporting weapons and ammunition to the occupation authorities, which are used by their army and terrorist settlers to kill Palestinian people and destroy their homes, hospitals, schools, mosques, churches and infrastructure. "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "\n",
        "- https://www.kaggle.com/code/dristantadas/yahoo-finance-scrapper-and-use-llm-starter\n",
        "- https://python.langchain.com/cookbook"
      ],
      "metadata": {
        "id": "BzzJTnDHmjHY"
      }
    }
  ]
}